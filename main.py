# -*- coding: utf-8 -*-
"""main.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Htf4TJ8sk0WfrnZ94wsTYq0pk2JtdCbC
"""

import tensorflow as tf

gpus = tf.config.list_physical_devices('GPU')
if gpus:
    try:
        for gpu in gpus:
            tf.config.experimental.set_memory_growth(gpu, True)
    except RuntimeError as e:
        print(e)

!pip install imblearn

!pip install pydot

!pip install graphviz

import os
import cv2
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
from tensorflow.keras.preprocessing.image import ImageDataGenerator
import keras
from keras.callbacks import EarlyStopping,ModelCheckpoint
import tensorflow as tf
from sklearn.metrics import confusion_matrix
from sklearn.metrics import classification_report
from tqdm import tqdm
from imblearn.over_sampling import SMOTE

dataset_path = r'D:\MLProjects\Alzi prediction\AugmentedAlzheimerDataset'

images = []
labels = []

for subfolder in tqdm(os.listdir(dataset_path)):
    subfolder_path = os.path.join(dataset_path, subfolder)
    if os.path.isdir(subfolder_path):  # Ensure it's a directory
        for folder in os.listdir(subfolder_path):
            subfolder_path2 = os.path.join(subfolder_path, folder)
            if os.path.isdir(subfolder_path2):  # Ensure it's a directory
                for image_filename in os.listdir(subfolder_path2):
                    image_path = os.path.join(subfolder_path2, image_filename)
                    images.append(image_path)
                    labels.append(folder)
            else:
                # It's a file, so handle it accordingly
                images.append(subfolder_path2)
                labels.append(subfolder)  # Use subfolder name as label

df = pd.DataFrame({'image': images, 'label': labels})
df

plt.figure(figsize=(15,8))
ax = sns.countplot(x=df.label,palette='Set1')
ax.set_xlabel("Class",fontsize=20)
ax.set_ylabel("Count",fontsize=20)
plt.title('The Number Of Samples For Each Class',fontsize=20)
plt.grid(True)
plt.xticks(rotation=45)
plt.show()

plt.figure(figsize=(50,50))
for n,i in enumerate(np.random.randint(0,len(df),50)):
    plt.subplot(10,5,n+1)
    img=cv2.imread(df.image[i])
    img=cv2.resize(img,(224,224))
    img=cv2.cvtColor(img,cv2.COLOR_BGR2RGB)
    plt.imshow(img)
    plt.axis('off')
    plt.title(df.label[i],fontsize=25)

# Apply LabelEncoder to the 'label' column in the dataframe
le = LabelEncoder()
df['label'] = le.fit_transform(df['label'])

# Check the mapping of classes to numeric values
print("Class mapping:", dict(zip(le.classes_, le.transform(le.classes_))))

Size = (176, 176)
work_dr = ImageDataGenerator(rescale=1./255)

# Ensure shuffle=True so that you don't get a single-class batch
train_data_gen = work_dr.flow_from_dataframe(
    df, x_col='image', y_col='label',
    target_size=Size,
    batch_size=6500,
    class_mode='raw',  # Raw mode to use the numeric labels directly
    shuffle=True  # Shuffling to ensure mixed batches
)

# Generate data
train_data, train_labels = train_data_gen.next()

# Check unique classes in the batch
unique_labels, counts = np.unique(train_labels, return_counts=True)
print("Unique classes in batch:", unique_labels)
print("Counts in batch:", counts)

class_num=np.sort(['AD','CN','EMCI','LMCI'])
class_num

from google.colab import drive
drive.mount('/content/drive')

unique_labels, counts = np.unique(train_labels, return_counts=True)
print("Unique classes:", unique_labels)
print("Counts:", counts)

# Reshape the data for SMOTE
train_data_reshaped = train_data.reshape(-1, 176 * 176 * 3)

# Apply SMOTE to balance the classes
sm = SMOTE(random_state=42)
train_data_resampled, train_labels_resampled = sm.fit_resample(train_data_reshaped, train_labels)

# Reshape the data back to its original shape
train_data_resampled = train_data_resampled.reshape(-1, 176, 176, 3)

print(train_data_resampled.shape, train_labels_resampled.shape)

# Assuming class_num is defined as:
class_num = np.array(['AD', 'CN', 'EMCI', 'LMCI'])

# Use the numeric labels to map back to class names
labels = [class_num[i] for i in train_labels_resampled]

# Plot the class distribution
plt.figure(figsize=(15,8))
ax = sns.countplot(x=labels, palette='Set1')
ax.set_xlabel("Class", fontsize=20)
ax.set_ylabel("Count", fontsize=20)
plt.title('The Number Of Samples For Each Class', fontsize=20)
plt.grid(True)
plt.xticks(rotation=45)
plt.show()

X_train, X_test1, y_train, y_test1 = train_test_split(train_data,train_labels, test_size=0.3, random_state=42,shuffle=True,stratify=train_labels)
X_val, X_test, y_val, y_test = train_test_split(X_test1,y_test1, test_size=0.5, random_state=42,shuffle=True,stratify=y_test1)
print('X_train shape is ' , X_train.shape)
print('X_test shape is ' , X_test.shape)
print('X_val shape is ' , X_val.shape)
print('y_train shape is ' , y_train.shape)
print('y_test shape is ' , y_test.shape)
print('y_val shape is ' , y_val.shape)

# Print the unique labels and their counts for the original dataset
unique_labels_original, counts_original = np.unique(train_labels, return_counts=True)
print("Original Unique classes:", unique_labels_original)
print("Counts in original dataset:", counts_original)

# Print the unique labels and their counts for the training labels
unique_labels_train, counts_train = np.unique(y_train, return_counts=True)
print("Training Unique classes:", unique_labels_train)
print("Counts in training dataset:", counts_train)

from sklearn.model_selection import train_test_split

# Splitting the data
X_train, X_test1, y_train, y_test1 = train_test_split(
    train_data,
    train_labels,
    test_size=0.3,
    random_state=42,
    shuffle=True,
    stratify=train_labels
)

X_val, X_test, y_val, y_test = train_test_split(
    X_test1,
    y_test1,
    test_size=0.5,
    random_state=42,
    shuffle=True,
    stratify=y_test1
)

print('After Splitting:')
print('X_train shape is ', X_train.shape)
print('y_train shape is ', y_train.shape)

from tensorflow.keras import mixed_precision

policy = mixed_precision.Policy('mixed_float16')
mixed_precision.set_global_policy(policy)

import tensorflow.keras.backend as K
import gc

K.clear_session()
gc.collect()

import numpy as np
import tensorflow as tf
from keras.layers import Dropout, GlobalAveragePooling2D, Flatten, Dense, BatchNormalization
from keras.applications import InceptionV3
from keras.models import Sequential
from keras.callbacks import ModelCheckpoint, EarlyStopping

base_model = tf.keras.applications.InceptionV3(input_shape=(176,176,3),include_top=False,weights='imagenet')
base_model.trainable = False
model_Inception=keras.models.Sequential()
model_Inception.add(base_model)
model_Inception.add(keras.layers.Dropout(.5))
model_Inception.add(keras.layers.GlobalAveragePooling2D())
model_Inception.add(keras.layers.Flatten())
model_Inception.add(keras.layers.BatchNormalization())
model_Inception.add(keras.layers.Dense(512,activation=tf.nn.relu))
model_Inception.add(keras.layers.BatchNormalization())
model_Inception.add(keras.layers.Dropout(.5))
model_Inception.add(keras.layers.Dense(256,activation=tf.nn.relu))
model_Inception.add(keras.layers.BatchNormalization())
model_Inception.add(keras.layers.Dropout(.5))
model_Inception.add(keras.layers.Dense(128,activation=tf.nn.relu))
model_Inception.add(keras.layers.BatchNormalization())
model_Inception.add(keras.layers.Dropout(.5))
model_Inception.add(keras.layers.Dense(64,activation=tf.nn.relu))
model_Inception.add(keras.layers.BatchNormalization())
model_Inception.add(keras.layers.Dropout(.5))
model_Inception.add(keras.layers.BatchNormalization())
model_Inception.add(keras.layers.Dense(4, activation=tf.nn.softmax))
model_Inception.summary()

# # Load Pretrained Model InceptionV3 without the top layer
# base_model = InceptionV3(input_shape=(176, 176, 3), include_top=False, weights='imagenet')

# # Define the Model Architecture
# model_Inception = Sequential([
#     base_model,
#     Dropout(0.5),
#     GlobalAveragePooling2D(),
#     BatchNormalization(),
#     Dense(512, activation='relu'),
#     BatchNormalization(),
#     Dropout(0.5),
#     Dense(256, activation='relu'),
#     BatchNormalization(),
#     Dropout(0.5),
#     Dense(128, activation='relu'),
#     BatchNormalization(),
#     Dropout(0.5),
#     Dense(4, activation='softmax')  # 4 classes for AD, CN, EMCI, LMCI
# ])

# Compile the Model
model_Inception.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.0001),
                        loss='sparse_categorical_crossentropy',
                        metrics=['accuracy'])

# Callbacks for early stopping and model checkpointing
checkpoint_cb = ModelCheckpoint("model_Inception.h5", save_best_only=True)
early_stopping_cb = EarlyStopping(patience=10, restore_best_weights=True)

# Convert data types to prevent issues during training (make sure they are numpy arrays first)
X_train = X_train.astype('float32')  # Replace `X_train` with actual training data
X_val = X_val.astype('float32')      # Replace `X_val` with actual validation data
y_train = y_train.astype('int32')    # Replace `y_train` with actual training labels
y_val = y_val.astype('int32')        # Replace `y_val` with actual validation labels

# Define a data generator to avoid loading the entire dataset into memory
def data_generator(X, y, batch_size):
    dataset_size = len(X)
    indices = np.arange(dataset_size)
    while True:
        np.random.shuffle(indices)
        for start_idx in range(0, dataset_size, batch_size):
            end_idx = min(start_idx + batch_size, dataset_size)
            excerpt = indices[start_idx:end_idx]
            yield X[excerpt], y[excerpt]

# Create training and validation datasets using the data generator
batch_size = 8
steps_per_epoch = len(X_train) // batch_size
validation_steps = len(X_val) // batch_size

train_dataset = tf.data.Dataset.from_generator(
    lambda: data_generator(X_train, y_train, batch_size),
    output_signature=(
        tf.TensorSpec(shape=(None, 176, 176, 3), dtype=tf.float32),
        tf.TensorSpec(shape=(None,), dtype=tf.int32)
    )
).prefetch(tf.data.AUTOTUNE)

val_dataset = tf.data.Dataset.from_generator(
    lambda: data_generator(X_val, y_val, batch_size),
    output_signature=(
        tf.TensorSpec(shape=(None, 176, 176, 3), dtype=tf.float32),
        tf.TensorSpec(shape=(None,), dtype=tf.int32)
    )
).prefetch(tf.data.AUTOTUNE)

# Train the Model
hist = model_Inception.fit(
    train_dataset,
    epochs=100,
    steps_per_epoch=steps_per_epoch,
    validation_data=val_dataset,
    validation_steps=validation_steps,
    callbacks=[checkpoint_cb, early_stopping_cb]
)

tf.keras.utils.plot_model(model_Inception, to_file='model.png', show_shapes=True, show_layer_names=True,show_dtype=True,dpi=120)

hist_=pd.DataFrame(hist.history)
hist_

plt.figure(figsize=(15,10))
plt.subplot(1,2,1)
plt.plot(hist_['loss'],label='Train_Loss')
plt.plot(hist_['val_loss'],label='Validation_Loss')
plt.title('Train_Loss & Validation_Loss',fontsize=20)
plt.legend()
plt.subplot(1,2,2)
plt.plot(hist_['accuracy'],label='Train_Accuracy')
plt.plot(hist_['val_accuracy'],label='Validation_Accuracy')
plt.title('Train_Accuracy & Validation_Accuracy',fontsize=20)
plt.legend()
plt.show()

score, acc= model_Inception.evaluate(X_test,y_test)
print('Test Loss =', score)
print('Test Accuracy =', acc)

# Ensure X_test and y_test have appropriate data types
X_test = X_test.astype('float32')  # Ensure test data is float32
y_test = y_test.astype('int32')    # Ensure test labels are int32

# Clear session to free GPU memory after training (if needed)
K.clear_session()

# Define a data generator for the test dataset
def test_data_generator(X, y, batch_size):
    dataset_size = len(X)
    indices = np.arange(dataset_size)
    while True:
        for start_idx in range(0, dataset_size, batch_size):
            end_idx = min(start_idx + batch_size, dataset_size)
            excerpt = indices[start_idx:end_idx]
            yield X[excerpt], y[excerpt]

# Batch size for the test dataset
batch_size = 4

# Create a TensorFlow Dataset from the generator
test_dataset = tf.data.Dataset.from_generator(
    lambda: test_data_generator(X_test, y_test, batch_size),
    output_signature=(
        tf.TensorSpec(shape=(None, 176, 176, 3), dtype=tf.float32),
        tf.TensorSpec(shape=(None,), dtype=tf.int32)
    )
).prefetch(tf.data.AUTOTUNE)

# Calculate steps per epoch
steps_per_epoch = len(X_test) // batch_size
print("Steps per epoch for test dataset:", steps_per_epoch)

# Predict using the model
predictions = model_Inception.predict(test_dataset, steps=steps_per_epoch)
y_pred = np.argmax(predictions, axis=1)

# Convert y_test to class labels if one-hot encoded
if len(y_test.shape) > 1:  # If y_test is one-hot encoded
    y_test_ = np.argmax(y_test, axis=1)
else:
    y_test_ = y_test

# Ensure lengths match before creating DataFrame
print(f"Length of actual labels: {len(y_test_)}")
print(f"Length of predicted labels: {len(y_pred)}")

# If they don't match, trim y_pred or y_test_ to match lengths
min_length = min(len(y_test_), len(y_pred))
y_test_ = y_test_[:min_length]
y_pred = y_pred[:min_length]

# Create a DataFrame with Actual and Predicted values
df = pd.DataFrame({'Actual': y_test_, 'Prediction': y_pred})
print(df)

plt.figure(figsize=(50,50))
for n,i in enumerate(np.random.randint(0,len(X_test),50)):
    plt.subplot(10,5,n+1)
    plt.imshow(X_test[i])
    plt.axis('off')
    plt.title(f'{class_num[y_test_[i]]} >>> {class_num[y_pred[i]]}',fontsize=15)

CM = confusion_matrix(y_test_,y_pred)
CM_percent = CM.astype('float') / CM.sum(axis=1)[:, np.newaxis]
sns.heatmap(CM_percent,fmt='g',center = True,cbar=False,annot=True,cmap='Blues')
CM

ClassificationReport = classification_report(y_test_, y_pred)

print("Classification Report:\n")
print(ClassificationReport)

