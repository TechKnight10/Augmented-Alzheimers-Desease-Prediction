# -*- coding: utf-8 -*-
"""DETECTIONofALZHEIMERwithMLKaggle.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Ibks_FFj-S7lbt2K_e8PJQ1SieNAylTO
"""

!pip install joblib

!pip install datasets

!pip install torchcam opencv-python matplotlib

!pip install torch torchvision torchaudio

#Checking the right version of tf
import tensorflow as tf

print(f"TensorFlow version: {tf.__version__}")
print("GPU Available:", tf.config.list_physical_devices('GPU'))

#checking the presence of gpu
with tf.device('/GPU:0'):
    # Your model code here
    model = tf.keras.Sequential([
        tf.keras.layers.Dense(128, activation='relu', input_shape=(784,)),
        tf.keras.layers.Dense(10, activation='softmax')
    ])
    model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])

!pip install tensorflow

!pip install datasets transformers pillow requests imblearn pydot graphviz joblib

import os
import zipfile
import datasets
from google.colab import files
import joblib
from tqdm import tqdm
from torchvision import transforms, models
import random
from PIL import Image
import shutil
import torch
import torch.nn as nn
import torchvision.models as models
from torchvision import datasets, transforms
from torch.utils.data import DataLoader, random_split, Subset
from sklearn.metrics import classification_report, confusion_matrix
import seaborn as sns
import matplotlib.pyplot as plt
import cv2
from torch.nn import functional as F
import numpy as np

# Step 1: Install Kaggle and Authenticate
!pip install -q kaggle

# Upload kaggle.json (API key file)

files.upload()

# Make a directory for the Kaggle API token
!mkdir -p ~/.kaggle
!mv kaggle.json ~/.kaggle/

# Set permissions
!chmod 600 ~/.kaggle/kaggle.json

# Step 2: Download the Augmented Alzheimer MRI Dataset
!kaggle datasets download -d uraninjo/augmented-alzheimer-mri-dataset

# Step 3: Extract the downloaded dataset
with zipfile.ZipFile('augmented-alzheimer-mri-dataset.zip', 'r') as zip_ref:
    zip_ref.extractall('alzheimer_augmented_dataset')

# Verify the extracted files
!ls alzheimer_augmented_dataset

# Step 4: Visualize the Directory Structure
for root, dirs, files in os.walk('alzheimer_augmented_dataset'):
    level = root.replace('alzheimer_augmented_dataset', '').count(os.sep)
    indent = ' ' * 4 * level
    print(f"{indent}{os.path.basename(root)}/")
    subindent = ' ' * 4 * (level + 1)
    for f in files[:5]:  # Limiting to the first 5 files per directory
        print(f"{subindent}{f}")

# Load the dataset using ImageFolder
data_dir = '/content/alzheimer_augmented_dataset/AugmentedAlzheimerDataset'  # Update the path if needed
full_dataset = datasets.ImageFolder(root=data_dir)

# Get the class-to-index mapping
class_to_idx = full_dataset.class_to_idx
print("Class to Index Mapping:", class_to_idx)

# Calculate the number of samples per class
class_counts = {class_name: 0 for class_name in class_to_idx}

for _, label in full_dataset:
    class_name = list(class_to_idx.keys())[list(class_to_idx.values()).index(label)]
    class_counts[class_name] += 1

# Print the number of samples in each class
for class_name, count in class_counts.items():
    print(f"{class_name}: {count} samples")

# Total number of samples
print(f"\nTotal samples: {len(full_dataset)}")

#Loading the Data set for the Model to train(70percente = train and 20 and 10 percent = test and val respectively)
torch.manual_seed(42)

# Define the data directories
train_data_dir = '/content/alzheimer_augmented_dataset/AugmentedAlzheimerDataset'
test_data_dir = '/content/alzheimer_augmented_dataset/OriginalDataset'

# Fixed number of samples per class for the training set
samples_per_class = 5500

# Transformations: Resize, Normalize, and Data Augmentation for training
train_transform = transforms.Compose([
    transforms.Resize((224, 224)),
    transforms.RandomHorizontalFlip(),
    transforms.RandomRotation(10),
    transforms.ToTensor(),
    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])
])

val_test_transform = transforms.Compose([
    transforms.Resize((224, 224)),
    transforms.ToTensor(),
    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])
])

def load_and_balance_data(data_dir, transform, fixed_samples=None):
    """Loads data from the given directory and balances classes."""
    dataset = datasets.ImageFolder(root=data_dir, transform=transform)
    class_indices = {i: [] for i in range(len(dataset.classes))}

    # Group samples by class
    for idx, (_, label) in enumerate(dataset):
        class_indices[label].append(idx)

    balanced_indices = []
    for indices in class_indices.values():
        # Use a fixed number of samples per class if specified
        if fixed_samples:
            sampled_indices = random.sample(indices, min(len(indices), fixed_samples))
        else:
            sampled_indices = indices
        balanced_indices.extend(sampled_indices)

    return Subset(dataset, balanced_indices)

# Load the augmented dataset for training with 3000 samples per class
train_dataset = load_and_balance_data(train_data_dir, train_transform, fixed_samples=samples_per_class)

# Load the original dataset for validation and testing without a fixed sample limit
full_original_dataset = load_and_balance_data(test_data_dir, val_test_transform)

# Calculate split sizes dynamically (70% training, 20% test, 10% validation)
test_size = int(0.2 * len(full_original_dataset))
val_size = int(0.1 * len(full_original_dataset))
remaining_size = len(full_original_dataset) - test_size - val_size

# Split the original dataset for validation and testing
val_dataset, test_dataset, _ = random_split(full_original_dataset, [val_size, test_size, remaining_size])

# Create DataLoaders
batch_size = 32
train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)
val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)
test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)

# Print dataset statistics
print(f"Training samples: {len(train_dataset)}")
print(f"Validation samples: {len(val_dataset)}")
print(f"Testing samples: {len(test_dataset)}")

#counting the no. of samples
from collections import Counter

# Get the labels from the training dataset
train_labels = [full_dataset[idx][1] for idx in train_dataset.indices]

# Count the occurrences of each class label
class_counts = Counter(train_labels)

print("Training class distribution:")
for class_name, count in class_counts.items():
    print(f"Class {class_name}: {count} samples")

#
#
#Finetuning the model with resnet 50
#
#
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print(f"Using device: {device}")

# Load a pre-trained ResNet50 model
model = models.resnet50(pretrained=True)

# Freeze the pre-trained layers to retain learned features
for param in model.parameters():
    param.requires_grad = False

# Unfreeze the last 2 residual blocks (layer4) for fine-tuning
for param in model.layer4.parameters():
    param.requires_grad = True

# Modify the final fully connected layer to match the number of classes (4)
num_classes = 4
model.fc = nn.Sequential(
    nn.Linear(model.fc.in_features, 512),  # Intermediate layer
    nn.ReLU(),
    nn.Dropout(0.3),  # Dropout to prevent overfitting
    nn.Linear(512, num_classes)  # Output layer for 4 classes
)

# Move the model to the device (GPU if available)
model = model.to(device)

# Enhanced Data Augmentation for Training
train_transforms = transforms.Compose([
    transforms.RandomRotation(15),
    transforms.RandomHorizontalFlip(),
    transforms.RandomVerticalFlip(),
    transforms.RandomResizedCrop(224, scale=(0.8, 1.0)),  # Random Cropping
    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),  # Color Jittering
    transforms.GaussianBlur(kernel_size=3, sigma=(0.1, 2.0)),  # Gaussian Noise
    transforms.ToTensor(),
    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])
])

# Validation and Test Transforms (no augmentation)
val_transforms = transforms.Compose([
    transforms.Resize((224, 224)),
    transforms.ToTensor(),
    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])
])

# Apply the transformations to the datasets
train_dataset.transform = train_transforms
val_dataset.transform = val_transforms

# Update optimizer to include unfrozen layers (layer4 and final layer)
optimizer = torch.optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), lr=0.001)

# Recreate DataLoaders
train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=32, shuffle=True)
val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=32, shuffle=False)

# Print the updated model architecture
print("Model Summary (with partially unfrozen layers):")
print(model)

#
#
#Training the model
#
#
criterion = torch.nn.CrossEntropyLoss()
num_epochs = 30
best_val_accuracy = 0.0

# Training and validation loop
for epoch in range(num_epochs):
    # Training Phase
    model.train()  # Set the model to training mode
    train_loss = 0.0
    correct_train = 0
    total_train = 0

    for images, labels in tqdm(train_loader, desc=f"Epoch {epoch+1}/{num_epochs} - Training"):
        images, labels = images.to(device), labels.to(device)

        # Forward pass
        outputs = model(images)
        loss = criterion(outputs, labels)

        # Backward pass and optimization
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()

        # Accumulate training loss
        train_loss += loss.item()

        # Calculate accuracy
        _, predicted = torch.max(outputs, 1)
        correct_train += (predicted == labels).sum().item()
        total_train += labels.size(0)

    train_accuracy = 100 * correct_train / total_train
    avg_train_loss = train_loss / len(train_loader)

    # Validation Phase
    model.eval()  # Set the model to evaluation mode
    val_loss = 0.0
    correct_val = 0
    total_val = 0

    with torch.no_grad():
        for images, labels in tqdm(val_loader, desc="Validation"):
            images, labels = images.to(device), labels.to(device)

            # Forward pass
            outputs = model(images)
            loss = criterion(outputs, labels)

            # Accumulate validation loss
            val_loss += loss.item()

            # Calculate accuracy
            _, predicted = torch.max(outputs, 1)
            correct_val += (predicted == labels).sum().item()
            total_val += labels.size(0)

    val_accuracy = 100 * correct_val / total_val
    avg_val_loss = val_loss / len(val_loader)

    # Print statistics
    print(f"Epoch [{epoch+1}/{num_epochs}], "
          f"Train Loss: {avg_train_loss:.4f}, Train Accuracy: {train_accuracy:.2f}%, "
          f"Val Loss: {avg_val_loss:.4f}, Val Accuracy: {val_accuracy:.2f}%")

    # Save the best model
    if val_accuracy > best_val_accuracy:
        best_val_accuracy = val_accuracy
        torch.save(model.state_dict(), "alzheimer_model.pth")
        print("Model improved, saved!")

print("Training complete. Best validation accuracy: {:.2f}%".format(best_val_accuracy))

# Evaluation on the test set
correct = 0
total = 0
all_preds = []
all_labels = []

with torch.no_grad():
    for images, labels in tqdm(test_loader, desc="Testing"):
        images, labels = images.to(device), labels.to(device)

        # Forward pass
        outputs = model(images)
        _, predicted = torch.max(outputs, 1)

        # Accumulate predictions and labels for metrics calculation
        all_preds.extend(predicted.cpu().numpy())
        all_labels.extend(labels.cpu().numpy())

        # Calculate accuracy
        correct += (predicted == labels).sum().item()
        total += labels.size(0)

# Calculate accuracy
test_accuracy = 100 * correct / total
print(f"Test Accuracy: {test_accuracy:.2f}%")

# Classification report
print("\nClassification Report:")
print(classification_report(all_labels, all_preds, target_names=['No Impairment', 'Very Mild Impairment', 'Mild Impairment', 'Moderate Impairment']))

# Confusion Matrix
cm = confusion_matrix(all_labels, all_preds)
plt.figure(figsize=(8, 8))
sns.heatmap(cm, annot=True, fmt="d", cmap="Blues",
            xticklabels=['No Impairment', 'Very Mild Impairment', 'Mild Impairment', 'Moderate Impairment'],
            yticklabels=['No Impairment', 'Very Mild Impairment', 'Mild Impairment', 'Moderate Impairment'])
plt.title('Confusion Matrix')
plt.xlabel('Predicted')
plt.ylabel('True')
plt.show()

model_filename = "alzheimer_model(98).pkl"
joblib.dump(model, model_filename)
print(f"âœ… Model saved as {model_filename}")

#Load the Model in Flask

model = joblib.load("alzheimer_model(99).pkl")
model.eval()

from google.colab import files
from PIL import Image
import torch
import torchvision.transforms as transforms
import torchvision.models as models

# Upload images
uploaded = files.upload()

# Load images and display filenames
for filename in uploaded.keys():
    print(f"Uploaded file: {filename}")

def preprocess_image(image_path):
    image = Image.open(image_path).convert("RGB")
    transform = transforms.Compose([
        transforms.Resize((224, 224)),
        transforms.ToTensor(),
        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])
    ])
    return transform(image).unsqueeze(0)

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print(f"Using device: {device}")
model = models.resnet50(pretrained=False)
num_classes = 4
model.fc = nn.Sequential(
    nn.Linear(model.fc.in_features, 512),
    nn.ReLU(),
    nn.Dropout(0.3),
    nn.Linear(512, num_classes)
)
model.load_state_dict(torch.load("alzheimer_model(98).pth"))
model = model.to(device)
model.eval()

# Reverse mapping
class_mapping = {
    0: "Mild Demented",
    1: "Moderate Demented",
    2: "Non Demented",
    3: "Very Mild Demented"
}

# Predict the uploaded images
for filename in uploaded.keys():
    image = preprocess_image(filename).to(device)
    with torch.no_grad():
        output = model(image)
        _, predicted = torch.max(output, 1)
        label = class_mapping[predicted.item()]
        print(f"Prediction for {filename}: {label}")

def display_image_with_prediction(image_path, label):
    image = Image.open(image_path).convert("RGB")
    plt.imshow(image)
    plt.title(f"Prediction: {label}")
    plt.axis("off")
    plt.show()

for filename in uploaded.keys():
    image = preprocess_image(filename).to(device)
    with torch.no_grad():
        output = model(image)
        _, predicted = torch.max(output, 1)
        label = class_mapping[predicted.item()]
        print(f"Prediction for {filename}: {label}")
        display_image_with_prediction(filename, label)